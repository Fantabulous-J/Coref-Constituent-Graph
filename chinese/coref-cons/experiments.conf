# Main configuration. Do not edit! Copy to experiments.conf and change that.
best {
  # Edit this
  data_dir = .
  model_type = independent
  # Computation limits.
  max_num_candidates = 3900
  max_top_antecedents = 50
  max_training_sentences = 5
  top_span_ratio = 0.4
  max_num_speakers = 20
  max_segment_len = 256

  # Learning
  bert_learning_rate = 1e-5
  task_learning_rate = 2e-4
  num_docs = 1810

  # Model hyperparameters.
  dropout_rate = 0.3
  ffnn_size = 1000
  ffnn_depth = 1
  num_epochs = 20
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = true
  coref_depth = 2
  coarse_to_fine = true
  fine_grained = true
  use_prior = true

  # Other.
  train_path = train.english.jsonlines
  eval_path = dev.english.jsonlines
  conll_eval_path = dev.english.v4_gold_conll
  single_example = true
  genres = ["bc", "bn", "mz", "nw", "tc", "wb"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = .
  adam_eps = 1e-6
  task_optimizer = adam
}

bert_base = ${best}{
  seed = 2333
  bert_learning_rate = 2e-05
  task_learning_rate = 0.0005
  max_segment_len = 384
  hidden_size = 768
  ffnn_size = 3000
  num_epochs = 40
  train_path = conll_data/train.chinese.384.jsonlines
  eval_path = conll_data/dev.chinese.384.jsonlines
  test_path = conll_data/test.chinese.384.jsonlines
  conll_dev_path = conll_data/dev.chinese.v4_gold_conll
  conll_test_path = conll_data/test.chinese.v4_gold_conll
  max_training_sentences = 3
  bert_config_file = bert_chinese_wwm_base/bert_config.json
  vocab_file = bert_chinese_wwm_base/vocab.txt
  best_checkpoint = train_bert_base_128/model_best.checkpoint
  tf_checkpoint = bert_base/model.max.ckpt
  init_checkpoint_dir = bert_chinese_wwm_base
  load_from_pretrained = true
  pretrained_mention = false
}

train_bert_base = ${bert_base}{
  mention_loss = true
  mention_loss_ratio = 10
  max_span_width = 30
  num_epochs = 40
  best_checkpoint = train_bert_base_384/model_best.checkpoint
  top_span_ratio = 0.3
  task_learning_rate = 0.0003
}

train_bert_base_mention_cons_dual_plus_multi_order2 = ${train_bert_base}{
  task_learning_rate = 0.0005
  num_epochs = 40
  num_heads = 4
  gat_dropout_rate = 0.2
  num_iterations = 2
  cons_hidden_size = 300
  cons_vocab_file = conll_data/constituent_gold_vocab.txt
  train_cons_path = conll_data/train.chinese.constituency.jsonlines
  eval_cons_path = conll_data/dev.chinese.constituency.jsonlines
  test_cons_path = conll_data/test.chinese.constituency.jsonlines
  best_checkpoint = train_bert_base_mention_cons_dual_plus_multi_order2_384/model_best.checkpoint
}

train_bert_base_mention_cons_dual_plus_multi_order2_4396 = ${train_bert_base_mention_cons_dual_plus_multi_order2}{
  seed = 4396
  best_checkpoint = train_bert_base_mention_cons_dual_plus_multi_order2_4396_384/model_best.checkpoint
}

train_bert_base_mention_cons_dual_plus_multi_order2_6324 = ${train_bert_base_mention_cons_dual_plus_multi_order2}{
  seed = 6324
  best_checkpoint = train_bert_base_mention_cons_dual_plus_multi_order2_6324_384/model_best.checkpoint
}

train_bert_base_mention_cons_dual_plus_multi_order2_9527 = ${train_bert_base_mention_cons_dual_plus_multi_order2}{
  seed = 9527
  best_checkpoint = train_bert_base_mention_cons_dual_plus_multi_order2_9527_384/model_best.checkpoint
}

train_bert_base_mention_cons_dual_plus_multi_order2_3527 = ${train_bert_base_mention_cons_dual_plus_multi_order2}{
  seed = 3527
  best_checkpoint = train_bert_base_mention_cons_dual_plus_multi_order2_3527_384/model_best.checkpoint
}

train_bert_base_mention_cons_bert_crf_dual_plus_multi_order2 = ${train_bert_base}{
  task_learning_rate = 0.0005
  num_epochs = 40
  num_heads = 4
  gat_dropout_rate = 0.2
  num_iterations = 2
  cons_hidden_size = 300
  cons_vocab_file = conll_data/constituent_bert_crf_vocab.txt
  train_cons_path = conll_data/train.chinese.constituency.bert.crf.jsonlines
  eval_cons_path = conll_data/dev.chinese.constituency.bert.crf.jsonlines
  test_cons_path = conll_data/test.chinese.constituency.bert.crf.jsonlines
  best_checkpoint = train_bert_base_mention_cons_bert_crf_dual_plus_multi_order2_384/model_best.checkpoint
}

train_bert_base_mention_cons_filter = ${train_bert_base_mention_cons_dual_plus_multi_order2}{
  best_checkpoint = train_bert_base_mention_cons_filter_384/model_best.checkpoint
}


roberta_large = ${best}{
  seed = 2333
  num_docs = 1809
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0003
  max_segment_len = 512
  hidden_size = 1024
  ffnn_size = 3000
  num_epochs = 60
  train_path = conll_data_roberta/train.chinese.512.jsonlines
  eval_path = conll_data_roberta/dev.chinese.512.jsonlines
  test_path = conll_data_roberta/test.chinese.512.jsonlines
  conll_dev_path = conll_data/dev.chinese.v4_gold_conll
  conll_test_path = conll_data/test.chinese.v4_gold_conll
  max_training_sentences = 3
  bert_config_file = chinese-roberta-wwm-ext-large/bert_config.json
  vocab_file = chinese-roberta-wwm-ext-large/vocab.txt
  tf_checkpoint = bert_large/model.max.ckpt
  init_checkpoint_dir = chinese-roberta-wwm-ext-large
  load_from_pretrained = true
  pretrained_mention = false
}

train_roberta_large = ${roberta_large}{
  coref_depth = 1
  mention_loss = true
  mention_loss_ratio = 10
  max_span_width = 30
  top_span_ratio = 0.4
  num_epochs = 60
  task_learning_rate = 0.0005
  best_checkpoint = train_bert_large_512/model_best.checkpoint
}

train_roberta_large_mention_cons_dual_plus_multi_order2 = ${train_roberta_large}{
  task_learning_rate = 0.0005
  num_epochs = 40
  num_heads = 8
  gat_dropout_rate = 0.2
  num_iterations = 2
  cons_hidden_size = 256
  top_span_ratio = 0.3
  cons_vocab_file = conll_data/constituent_gold_vocab.txt
  train_cons_path = conll_data/train.chinese.constituency.jsonlines
  eval_cons_path = conll_data/dev.chinese.constituency.jsonlines
  test_cons_path = conll_data/test.chinese.constituency.jsonlines
  best_checkpoint = train_roberta_large_mention_cons_dual_plus_multi_order2_512/model_best.checkpoint
}

train_roberta_large_mention_cons_dual_plus_multi_order2_0.35top_span_ratio = ${train_roberta_large_mention_cons_dual_plus_multi_order2}{
  top_span_ratio = 0.35
}

train_roberta_large_mention_cons_dual_plus_multi_order2_6324 = ${train_roberta_large_mention_cons_dual_plus_multi_order2}{
  seed = 6324
  best_checkpoint = train_roberta_large_mention_cons_dual_plus_multi_order2_6324_512/model_best.checkpoint
}

train_roberta_large_mention_cons_dual_plus_multi_order2_4396 = ${train_roberta_large_mention_cons_dual_plus_multi_order2}{
  seed = 4396
  best_checkpoint = train_roberta_large_mention_cons_dual_plus_multi_order2_4396_512/model_best.checkpoint
}

train_roberta_large_mention_cons_dual_plus_multi_order2_9527 = ${train_roberta_large_mention_cons_dual_plus_multi_order2}{
  seed = 9527
  best_checkpoint = train_roberta_large_mention_cons_dual_plus_multi_order2_9527_512/model_best.checkpoint
}

train_roberta_large_mention_cons_dual_plus_multi_order2_3527 = ${train_roberta_large_mention_cons_dual_plus_multi_order2}{
  seed = 3527
  best_checkpoint = train_roberta_large_mention_cons_dual_plus_multi_order2_3527_512/model_best.checkpoint
}
